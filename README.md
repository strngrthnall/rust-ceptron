<h1 align="center">ğŸ§  Rust-ceptron (Perceptron em Rust)</h1>

<p align="center">
  <img src="https://img.shields.io/badge/Rust-000000?style=for-the-badge&logo=rust&logoColor=white" alt="Rust">
  <img src="https://img.shields.io/badge/Machine%20Learning-FF6F00?style=for-the-badge&logo=tensorflow&logoColor=white" alt="Machine Learning">
  <img src="https://img.shields.io/badge/Status-Em%20Desenvolvimento-yellow?style=for-the-badge" alt="Status">
</p>


<p align="center">
  <strong>ImplementaÃ§Ã£o de um Perceptron do zero, sem frameworks de Machine Learning</strong>
</p>

<p align="center">
  <a href="#-sobre">Sobre</a> â€¢
  <a href="#-motivaÃ§Ã£o">MotivaÃ§Ã£o</a> â€¢
  <a href="#-funcionalidades">Funcionalidades</a> â€¢
  <a href="#-estrutura">Estrutura</a> â€¢
  <a href="#-como-executar">Como Executar</a> â€¢
  <a href="#-referÃªncias">ReferÃªncias</a> â€¢
  <a href="#-licenÃ§a">LicenÃ§a</a>
</p>

---

## ğŸ“– Sobre

Este projeto Ã© uma implementaÃ§Ã£o de um **Perceptron** em **Rust**, baseado nas aulas do canal **[Do Zero](https://www.youtube.com/@dozero)** no YouTube, onde a implementaÃ§Ã£o original Ã© feita em **C**.

O objetivo principal Ã© **aprender os fundamentos de redes neurais** construindo tudo do zero, sem depender de bibliotecas de Machine Learning como TensorFlow ou PyTorch. Aqui, optamos por reescrever o projeto em **Rust** para explorar as vantagens da linguagem em termos de seguranÃ§a de memÃ³ria e performance.

### Como funciona

O perceptron implementado Ã© capaz de aprender funÃ§Ãµes lineares com **mÃºltiplas entradas** atravÃ©s do algoritmo de **gradiente descendente**:

1. **InicializaÃ§Ã£o**: Pesos (um para cada entrada) e bias sÃ£o inicializados com valores aleatÃ³rios
2. **Forward Pass**: Calcula a saÃ­da do neurÃ´nio: `y = f(Î£(xáµ¢ Ã— wáµ¢) + bias)`
3. **CÃ¡lculo do Custo**: Mede o erro usando MSE (Mean Squared Error)
4. **CÃ¡lculo do Gradiente**: Usa diferenÃ§as finitas para aproximar a derivada parcial de cada parÃ¢metro
5. **AtualizaÃ§Ã£o**: Ajusta todos os pesos e o bias na direÃ§Ã£o que reduz o erro

**Exemplo atual**: O neurÃ´nio aprende a relaÃ§Ã£o entre entradas e saÃ­das a partir de um conjunto de dados de treinamento.

> âš ï¸ **Nota:** Este Ã© um projeto de **estudo** e nÃ£o deve ser utilizado em produÃ§Ã£o. O foco estÃ¡ no aprendizado dos conceitos fundamentais de redes neurais artificiais.

---

## ğŸ¯ MotivaÃ§Ã£o

- ğŸ“š **Aprendizado**: Compreender os conceitos fundamentais de redes neurais na prÃ¡tica
- ğŸ¦€ **Rust**: Praticar a linguagem Rust em um contexto de Machine Learning
- ğŸ”§ **Do Zero**: Implementar sem abstraÃ§Ãµes para entender "por baixo do capÃ´"
- ğŸ¥ **InspiraÃ§Ã£o**: Acompanhar e adaptar o conteÃºdo do canal Do Zero para Rust

---


## âœ¨ Funcionalidades

- [x] Estrutura bÃ¡sica do NeurÃ´nio (Perceptron)
- [x] Suporte a mÃºltiplas entradas (n conexÃµes)
- [x] InicializaÃ§Ã£o de pesos e bias aleatÃ³rios
- [x] FunÃ§Ãµes de ativaÃ§Ã£o (Identidade, Sigmoid, ReLU)
- [x] ComputaÃ§Ã£o de saÃ­da do neurÃ´nio
- [x] FunÃ§Ã£o de custo MSE (Mean Squared Error)
- [x] CÃ¡lculo de gradiente por diferenÃ§as finitas
- [x] Algoritmo de treinamento (Gradiente Descendente)
- [x] Suporte a mÃºltiplas camadas (MLP - Multi-Layer Perceptron) via campo `con_neurons`
- [ ] Mais funÃ§Ãµes de ativaÃ§Ã£o (Tanh)

---

## ğŸ“ Estrutura

```
perceptron/
â”œâ”€â”€ Cargo.toml          # ConfiguraÃ§Ã£o do projeto e dependÃªncias
â”œâ”€â”€ README.md           # DocumentaÃ§Ã£o do projeto
â””â”€â”€ src/
    â”œâ”€â”€ main.rs         # Ponto de entrada e demonstraÃ§Ã£o de treinamento
    â”œâ”€â”€ neuron.rs       # Estrutura do neurÃ´nio e funÃ§Ãµes de inicializaÃ§Ã£o
    â”œâ”€â”€ neuralnet.rs    # FunÃ§Ãµes de treinamento e cÃ¡lculo de custo
    â”œâ”€â”€ netmath.rs      # FunÃ§Ãµes matemÃ¡ticas (ativaÃ§Ã£o, MSE)
    â””â”€â”€ utils.rs        # UtilitÃ¡rios (geraÃ§Ã£o de nÃºmeros aleatÃ³rios)
```

### MÃ³dulos

| MÃ³dulo | DescriÃ§Ã£o |
|--------|----------|
| `main.rs` | Ponto de entrada, define dados de treinamento e executa o loop de treinamento |
| `neuron.rs` | Define a estrutura `Neuron` e funÃ§Ãµes `init_neuron()` e `compute_out()` |
| `neuralnet.rs` | Implementa `compute_cost()`, `compute_gradient()` e `train()` |
| `netmath.rs` | FunÃ§Ãµes de ativaÃ§Ã£o (`ident`, `sigmoid`, `relu`) e custo (`mse`) |
| `utils.rs` | FunÃ§Ã£o `randomize()` para gerar valores aleatÃ³rios |

### Componentes Principais

| Componente | MÃ³dulo | DescriÃ§Ã£o |
|------------|--------|----------|
| `Neuron` | `neuron.rs` | Estrutura que representa um neurÃ´nio com pesos, bias, funÃ§Ã£o de ativaÃ§Ã£o e conexÃµes para mÃºltiplas camadas |
| `init_neuron()` | `neuron.rs` | Inicializa um neurÃ´nio com pesos e bias aleatÃ³rios |
| `compute_out()` | `neuron.rs` | Calcula a saÃ­da do neurÃ´nio dado um vetor de entrada ou saÃ­da de outros neurÃ´nios |
| `mse()` | `netmath.rs` | Calcula o erro quadrÃ¡tico mÃ©dio (Mean Squared Error) |
| `ident()` | `netmath.rs` | FunÃ§Ã£o de ativaÃ§Ã£o identidade (f(x) = x) |
| `sigmoid()` | `netmath.rs` | FunÃ§Ã£o de ativaÃ§Ã£o sigmoid (Ïƒ(x) = 1/(1 + eâ»Ë£)) |
| `relu()` | `netmath.rs` | FunÃ§Ã£o de ativaÃ§Ã£o ReLU (Rectified Linear Unit) |
| `compute_cost()` | `neuralnet.rs` | Calcula o custo total do neurÃ´nio para um conjunto de amostras |
| `compute_gradient()` | `neuralnet.rs` | Calcula o gradiente de um parÃ¢metro usando diferenÃ§as finitas |
| `train()` | `neuralnet.rs` | Treina o neurÃ´nio usando gradiente descendente |
| `Net` | `neuralnet.rs` | Estrutura que representa uma rede neural multicamada (MLP), contendo neurÃ´nios de saÃ­da, funÃ§Ãµes de ativaÃ§Ã£o e mÃ©todo construtor |
| `Net::new()` | `neuralnet.rs` | Construtor que inicializa uma rede neural com mÃºltiplas camadas e funÃ§Ãµes de ativaÃ§Ã£o personalizadas |
| `randomize()` | `utils.rs` | Gera valores aleatÃ³rios em um intervalo |

---

## ğŸš€ Como Executar

### PrÃ©-requisitos

- [Rust](https://www.rust-lang.org/tools/install) instalado (versÃ£o 1.70+)
- Cargo (gerenciador de pacotes do Rust)

### InstalaÃ§Ã£o

```bash
# Clone o repositÃ³rio
git clone https://github.com/strngrthnall/perceptron.git

# Entre no diretÃ³rio
cd perceptron

# Compile o projeto
cargo build --release

# Execute
cargo run
```

### SaÃ­da Esperada

O programa treina um neurÃ´nio com **2 entradas** para aprender a relaÃ§Ã£o entre entradas e saÃ­das:

```
***Antes do treinamento***
O custo do neurÃ´nio : 42.5     (valor varia conforme inicializaÃ§Ã£o aleatÃ³ria)
O valor do weight 1 : 0.42     (peso aleatÃ³rio)
O valor do weight 2 : -0.31    (peso aleatÃ³rio)
O valor do bias     : -0.78    (bias aleatÃ³rio)

***Depois do treinamento***
O custo do neurÃ´nio : ~0.01    (erro mÃ­nimo)
O valor do weight 1 : ~0.5     (peso ajustado)
O valor do weight 2 : ~0.1     (peso ajustado)
O valor do bias     : ~0.8     (bias ajustado)

*** Testes ***
Entrada 1 5 - SaÃ­da ~3.2
Entrada 2 8 - SaÃ­da ~4.5
Entrada 4 6 - SaÃ­da ~5.0
Entrada 5 9 - SaÃ­da ~6.8
Entrada 9 8 - SaÃ­da ~8.2
Entrada 8 5 - SaÃ­da ~6.0
```

> ğŸ’¡ Os valores iniciais sÃ£o aleatÃ³rios, mas apÃ³s 50.000 iteraÃ§Ãµes de treinamento,
> o neurÃ´nio converge para parÃ¢metros que minimizam o erro entre prediÃ§Ãµes e valores esperados.

---

## ğŸ“š ReferÃªncias

- ğŸ¥ **Canal Do Zero** - [YouTube](https://www.youtube.com/@dozero)
  - SÃ©rie de vÃ­deos sobre implementaÃ§Ã£o de redes neurais em C
- ï¿½ **RepositÃ³rio Original (C)** - [GitHub](https://github.com/acsfranco/dozero)
  - ImplementaÃ§Ã£o original em C do canal Do Zero
- ï¿½ğŸ“– **DocumentaÃ§Ã£o Rust** - [rust-lang.org](https://doc.rust-lang.org/book/)
- ğŸ§  **Perceptron** - [Wikipedia](https://en.wikipedia.org/wiki/Perceptron)

---

## ğŸ› ï¸ Tecnologias

| Tecnologia | VersÃ£o | Uso |
|------------|--------|-----|
| Rust | 2024 Edition | Linguagem principal |
| rand | 0.8 | GeraÃ§Ã£o de nÃºmeros aleatÃ³rios |
| num | 0.4.3 | OperaÃ§Ãµes matemÃ¡ticas |

---

## ğŸ“ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo [LICENSE](LICENSE) para mais detalhes.

---

## ğŸ¤ ContribuiÃ§Ã£o

ContribuiÃ§Ãµes sÃ£o bem-vindas! Este Ã© um projeto de estudo, entÃ£o sinta-se Ã  vontade para:

1. Fazer um Fork do projeto
2. Criar uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Add some AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abrir um Pull Request

---

---

## â„¹ï¸ ObservaÃ§Ã£o sobre uso de IA

Os comentÃ¡rios do cÃ³digo e este README foram gerados ou revisados com auxÃ­lio de InteligÃªncia Artificial (IA), sem alteraÃ§Ã£o da lÃ³gica ou implementaÃ§Ã£o do cÃ³digo-fonte.

---

<p align="center">
  Feito com â¤ï¸ para fins educacionais
</p>
<p align="center">
  <sub>Inspirado nas aulas do canal <a href="https://www.youtube.com/@dozero">Do Zero</a></sub>
</p>
