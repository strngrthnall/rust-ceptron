/*
 * neuralnet.rs
 *
 * Módulo de rede neural contendo funções de treinamento.
 *
 * Este módulo implementa:
 *   - Cálculo de custo total do neurônio
 *   - Cálculo de gradientes por diferenças finitas
 *   - Algoritmo de treinamento por gradiente descendente
 */


use crate::neuron::*;

/*
 * Calcula o custo total do neurônio para um conjunto de amostras.
 *
 * Parâmetros:
 *   neuron - referência ao neurônio a ser avaliado
 *   x - vetor de vetores contendo as entradas de cada amostra
 *   y - vetor com os valores esperados (gabarito)
 *   cost - função de custo a ser utilizada (ex: mse)
 *   sample_size - número de amostras
 *
 * Retorno:
 *   O custo calculado pela função de custo fornecida
 */

pub fn compute_cost(
    neuron: &Neuron, 
    x: &Vec<Vec<f32>>, 
    y: &[f32], 
    cost: fn(&[f32], &[f32], usize) -> f32, 
    sample_size: usize
) -> f32 {
    let mut out_pred: Vec<f32> = Vec::new();

    for i in 0..sample_size {
        out_pred.push(neuron.compute_out(&x[i]));
    }
    cost(y, &out_pred, sample_size)
}

/*
 * Enum que representa o tipo de parâmetro a ser ajustado.
 *
 * Variantes:
 *   Weight(usize) - representa um peso específico pelo seu índice
 *   Bias - representa o bias do neurônio
 */

enum ParamType {
    Weight(usize),
    Bias,
}
/*
 * Calcula o gradiente de um parâmetro usando diferenças finitas.
 *
 * Utiliza a aproximação numérica da derivada:
 *   lim (delta -> 0) [cost(param + delta) - cost(param)] / delta
 *
 * Parâmetros:
 *   neuron - referência mutável ao neurônio
 *   cost - função de custo a ser utilizada (ex: mse)
 *   x - vetor de vetores contendo as entradas de cada amostra
 *   y - vetor com os valores esperados (gabarito)
 *   param - tipo do parâmetro a ser derivado (Weight ou Bias)
 *   sample_size - número de amostras
 *
 * Retorno:
 *   O gradiente (derivada parcial) do parâmetro em relação ao custo
 */
fn compute_gradient(
    neuron: &mut Neuron, 
    cost: fn(&[f32], &[f32], usize) -> f32,
    x: &Vec<Vec<f32>>,
    y: &[f32],
    param: ParamType,
    sample_size: usize
) -> f32 {
    let eps = 0.0001;
    
    // Modifica o parâmetro diretamente no neurônio
    match param {
        ParamType::Weight(i) => neuron.weights[i] += eps,
        ParamType::Bias => neuron.bias += eps,
    }
    let variation_cost = compute_cost(neuron, x, y, cost, sample_size);
    
    // Restaura o parâmetro
    match param {
        ParamType::Weight(i) => neuron.weights[i] -= eps,
        ParamType::Bias => neuron.bias -= eps,
    }
    let normal_cost = compute_cost(neuron, x, y, cost, sample_size);

    (variation_cost - normal_cost) / eps
}

/*
 * Treina o neurônio ajustando seus pesos e bias.
 *
 * Utiliza o algoritmo de gradiente descendente para minimizar
 * a função de custo, atualizando os parâmetros iterativamente.
 *
 * Parâmetros:
 *   neuron - referência ao neurônio a ser treinado
 *   cost - função de custo a ser minimizada (ex: mse)
 *   x - vetor de vetores contendo as entradas de cada amostra
 *   y - vetor com os valores esperados (gabarito)
 *   sample_size - número de amostras
 *
 * Retorno:
 *   Nenhum (modifica o neurônio in-place)
 */

pub fn train(
    neuron: &mut Neuron, 
    cost: fn(&[f32], &[f32], usize) -> f32, 
    x: &Vec<Vec<f32>>, 
    y: &[f32], 
    sample_size: usize
) {
    let mut gradient;

    for i in 0..neuron.n_connections as usize {
        let param = ParamType::Weight(i);
        gradient = compute_gradient(neuron, cost, x, y, param, sample_size);
        neuron.weights[i] -= 0.001 * gradient;
    }

    let param = ParamType::Bias;
    gradient = compute_gradient(neuron, cost, x, y, param, sample_size);
    neuron.bias -= 0.001 * gradient;

}



/*
 * Estrutura Net
 *
 * Representa uma rede neural multicamada (MLP), contendo:
 *   - Vetor de neurônios de saída
 *   - Número de neurônios de saída
 *   - Função de ativação interna
 *   - Função de ativação de saída
 */
#[derive(Clone)]
struct Net {
    out_neurons: Vec<Neuron>,
    n_out: u16,
    int_act_func: fn(f32) -> f32,
    out_act_func: fn(f32) -> f32,
}

impl Net {

    /*
     * Construtor da estrutura Net
     *
     * Inicializa uma rede neural multicamada (MLP) com o número de camadas e neurônios
     * especificados em `layers`, além das funções de ativação internas e de saída.
     *
     * Parâmetros:
     *   layers - vetor contendo o número de neurônios por camada
     *   int_act_func - função de ativação para camadas internas
     *   out_act_func - função de ativação para camada de saída
     *
     * Retorno:
     *   Instância da estrutura Net
     */
    pub fn new(
        layers: Vec<u16>, 
        int_act_func: fn(f32) -> f32,
        out_act_func: fn(f32) -> f32,
    ) -> Self {
    
        let n_layers = layers.len();

        let mut prev_layer: Vec<Neuron> = Vec::new();
        let n_out = layers[n_layers - 1];

        for i in 1..n_layers {
            let n_connections = layers[i - 1];
            let n_neurons = layers[i];
            
            let mut curr_layer: Vec<Neuron> = Vec::new();

            for _j in 0..n_neurons {
                let neuron = Neuron::new(
                    if i < n_layers - 1 { int_act_func } else { out_act_func }, 
                    n_connections as u32,
                    prev_layer.clone()
                );

                curr_layer.push(neuron);
            }

            prev_layer = curr_layer;
        }

        Net {
            out_neurons: prev_layer,
            n_out,
            int_act_func,
            out_act_func,
        }

    }
}